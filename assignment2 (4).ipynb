{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2:  the perceptron\n",
    "\n",
    "Due date:  Friday 9/21 at 11:59pm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Namita Kharat**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Datasets\n",
    "\n",
    "In this assignment we will use the following datasets:\n",
    "  * The [Gisette](http://archive.ics.uci.edu/ml/datasets/Gisette) handwritten digit recognition dataset. \n",
    "  * The [QSAR](http://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation) data for predicting the biochemical activity of a molecule.\n",
    "  * The [Heart disease diagnosis](http://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset.\n",
    "  * For developing your code, you can use one of the scikit-learn datasets, such as the [breast cancer wisconsin dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) and the [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) toy dataset generator.\n",
    "  \n",
    "When writing your notebook, you can assume the datasets are in the same directory as the notebook.  Please keep the same file names as in the UCI repository.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Variants of the perceptron algorithm \n",
    "\n",
    "In this assignment you will work with several variants of the perceptron algorithm:\n",
    "\n",
    "  * The \"vanila\" version of the perceptron algorithm, which was introduced in class.\n",
    "  * The pocket algorithm as described in the slides or page 80 in the book.\n",
    "  * The **adatron** version of the perceptron described next.\n",
    "\n",
    "In each case make sure that your implementation of the classifier **includes a bias term** (in slide set 2 and page 7 in the book you will find guidance on how to add a bias term to an algorithm that is expressed without one).\n",
    "\n",
    "## The adatron \n",
    "\n",
    "Before we get to the adatron, we will derive an alternative form of the perceptron algorithm --- the dual perceptron algorithm.  All we need to look at is the weight update rule:\n",
    "\n",
    "$$\\mathbf{w} \\rightarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i.$$\n",
    "\n",
    "This is performed whenever example $i$ is misclassified by the current weight vector.  The thing to notice, is that the weight vector is always a weighted combination of the training examples since it is that way to begin with, and each update maintains that property.  So in fact, rather than representing $\\mathbf{w}$ explicitly, all we need to do is to keep track of how much each training example is contributing to the value of the weight vector, i.e. we will express it as:\n",
    "\n",
    "$$\\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i,$$\n",
    "\n",
    "where $\\alpha_i$ are positive numbers that describe the magnitude of the contribution $\\mathbf{x}_i$ is making to the weight vector, and $N$ is the number of training examples.\n",
    "\n",
    "Therefore to initialize $\\mathbf{w}$ to 0, we simply initialize $\\alpha_i = 0$ for $i = 1,\\ldots,N$.  When expressed using the variables $\\alpha_i$, the perceptron update rule becomes:\n",
    "\n",
    "$$\\alpha_i = \\alpha_i + \\eta y_i,$$\n",
    "\n",
    "and you can always retrieve the weight vector using its expansion in terms of the $\\alpha_i$.\n",
    "\n",
    "Now we're ready for the adatron - the only difference is in the initialization and update equation.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$\\alpha_i = 1$ for $i = 1,\\ldots,N$\n",
    "\n",
    "Like in the perceptron we run the algorithm until convergence, or until a fixed number of epochs has passed (an epoch is a loop over all the training data), and an epoch of training consists of the following procedure:\n",
    "\n",
    "for each training example $i=1,\\ldots,N$ perform the following steps:\n",
    "\n",
    "1.  $\\gamma = y_i * \\mathbf{w}^{t} \\mathbf{x}_i$\n",
    "2.  $\\delta\\alpha = \\eta * (1 - \\gamma)$\n",
    "3.  `if` $(\\alpha_i + \\delta\\alpha < 0)$ : $\\alpha_i = 0$, `else : ` $\\alpha_i = \\alpha_i + \\delta\\alpha$\n",
    "\n",
    "\n",
    "The variable $\\eta$ plays the role of the learning rate $\\eta$ employed in the perceptron algorithm and $\\delta \\alpha$ is the proposed magnitude of change in $\\alpha_i$. \n",
    "We note that the adatron tries to maintain a **sparse** representation in terms of the training examples by keeping many $\\alpha_i$ equal to zero.  The adatron converges to a special case of the SVM algorithm that we will learn later in the semester; this algorithm tries to maximize the margin with which each example is classified, which is captured by the variable $\\gamma$ in the algorithm (notice that the magnitude of change proposed for each $\\alpha_i$ becomes smaller as $\\gamma$ increases towards 1).\n",
    "\n",
    "**Note:** if you observe an overflow issues in running the adatron, add an upper bound on the value of $\\alpha_i$.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "  - Implement the pocket algorithm and the adatron; each classifier should be implemented in a separate Python class, and use the same interface used in the code provided for the perceptron algorithm, i.e. provides the same methods with the same signature.  Make sure each classifier you use (including the original version of the perceptron) implements a bias term.\n",
    "  - Compare the performance of these variants of the perceptron on the Gisette and QSAR datasets by computing an estimate of the out of sample error on a sample of the data that you reserve for testing (the test set).  In each case reserve about 60% of the data for training, and 40% for testing.  To gain more confidence in our error estimates, repeat this experiment using 10 random splits of the data into training/test sets.  Report the average error and its standard deviation in a nicely formatted table.  Is there a version of the perceptron that appears to perform better?   (In answering this, consider the differences in performance you observe in comparison to the standard deviation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron :\n",
    " \n",
    "    \"\"\"An implementation of the perceptron algorithm.\n",
    "    This implementation includes a bias term\"\"\"\n",
    " \n",
    "    def __init__(self, max_iterations=100, learning_rate=0.2) :\n",
    " \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    " \n",
    "    def fit(self, X, y) :\n",
    "        \"\"\"\n",
    "        Train a classifier using the perceptron training algorithm.\n",
    "        After training the attribute 'w' will contain the perceptron weight vector.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    " \n",
    "        X : ndarray, shape (num_examples, n_features)\n",
    "        Training data.\n",
    " \n",
    "        y : ndarray, shape (n_examples,)\n",
    "        Array of labels.\n",
    " \n",
    "        \"\"\"\n",
    "        ones = np.ones((np.size(X, 0), 1))       # Vector of all 1's\n",
    "        X = np.append(ones, X, 1)                # Bias term added to the X(Training data)\n",
    "        self.w = np.zeros(len(X[0]))             # Initializing the weight vector\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        while (not converged and iterations <= self.max_iterations) :  # if misclassified\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 :\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]  #update w\n",
    "                    converged = False\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    " \n",
    "    def decision_function(self, X) :    #returns the dot product of weight vector and X(Training Data)\n",
    "        return np.inner(self.w, X)\n",
    " \n",
    "    def predict(self, X) :  # returns score of X\n",
    "        \"\"\"\n",
    "        make predictions using a trained linear classifier\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    " \n",
    "        X : ndarray, shape (num_examples, n_features)\n",
    "        Training data.\n",
    "        \"\"\"\n",
    "        ones = np.ones((np.size(X, 0), 1))\n",
    "        X = np.append(ones, X, 1)\n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocket Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pocket:\n",
    "    def __init__(self, max_iterations=100, learning_rate=0.2):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ones = np.ones((np.size(X, 0), 1))            # Vector of all 1's\n",
    "        X = np.append(ones, X, 1)                     # Bias term added to X\n",
    "        self.w = np.zeros(len(X[0]))                  # Initializing the weight vector\n",
    "        self.w_pocket = np.zeros(len(X[0]))           # To store the best weights with minimum error    \n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        while (not converged and iterations <= self.max_iterations) : # if misclassified\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 : \n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]  #update w\n",
    "                    converged = False\n",
    "                    # update pocket w if result is better than previous pocket w\n",
    "                    if (self.calculate_error(self.w_pocket, X, y) > self.calculate_error(self.w, X, y)):\n",
    "                        self.w_pocket = np.copy(self.w)\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "    \n",
    "    def calculate_error(self, w, X, y):  # returns the calculated error\n",
    "        scores = np.inner(w, X)\n",
    "        error = (np.sum(np.sign(scores) != y))/len(y)        \n",
    "        return error\n",
    "    \n",
    "    def decision_function(self, x) : # returns the dot product of weight vector and X(Training Data)\n",
    "        return np.inner(self.w, x)\n",
    "    \n",
    "    def predict(self, X) : # returns score of X        \n",
    "        ones = np.ones((np.size(X, 0), 1))\n",
    "        X = np.append(ones, X, 1)        \n",
    "        scores = np.inner(self.w_pocket, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adatron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adatron:\n",
    "    def __init__(self, max_iterations=100, learning_rate=0.2):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ones = np.ones((np.size(X, 0), 1))                  # Vector of all 1's\n",
    "        X = np.append(ones, X, 1)                           # Adding the Bias term\n",
    "        self.alpha = np.ones((np.size(X, 0)))               # Initializing alpha vector with Nx1 dimensions\n",
    "        self.w = np.zeros(len(X[0]))                        # Initializing the weight vector\n",
    "        iterations = 0\n",
    "        while (iterations <= self.max_iterations):\n",
    "            for i in range(len(X)):\n",
    "                gamma = y[i] * self.decision_function(X[i])   \n",
    "                delta_alpha = self.learning_rate * (1-gamma)  \n",
    "                if (self.alpha[i] + delta_alpha < 0):       # Updating alpha\n",
    "                    self.alpha[i] = 0\n",
    "                else:\n",
    "                    self.alpha[i] = self.alpha[i] + delta_alpha                \n",
    "                if (self.alpha[i] > 5000):\n",
    "                    self.alpha[i] = 5000                    \n",
    "                self.w = self.w + (self.alpha[i] * y[i] * X[i])      # Updating weight vector       \n",
    "            iterations += 1\n",
    "    \n",
    "    def decision_function(self, x) : # magnitude of dot/inner product btw w, X\n",
    "        return np.inner(self.w, x)\n",
    "    \n",
    "    def predict(self, X) : # returns score of X\n",
    "        ones = np.ones((np.size(X, 0), 1))\n",
    "        X = np.append(ones, X, 1)        \n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read gisette data\n",
    "def read_gisette_data(traininglabels, validlabels, trainingdata, validdata):\n",
    "    # load data from files\n",
    "    training_labels = np.genfromtxt(traininglabels, delimiter=\"\\n\")\n",
    "    valid_labels = np.genfromtxt(validlabels, delimiter=\"\\n\")\n",
    "    training_data = np.genfromtxt(trainingdata, delimiter=\" \")\n",
    "    valid_data = np.genfromtxt(validdata, delimiter=\" \")\n",
    "    # adding labels to data\n",
    "    training_data_labels = np.append(training_data, training_labels[:,None], axis=1)\n",
    "    valid_data_labels = np.append(valid_data, valid_labels[:,None], axis=1)\n",
    "    # concatinating all data to get the complete dataset\n",
    "    complete_data = np.concatenate((training_data_labels, valid_data_labels), axis=0)\n",
    "    return complete_data\n",
    "\n",
    "def gisette_data():\n",
    "    gisettedata = read_gisette_data(\"gisette_train.labels\", \"gisette_valid.labels\", \"gisette_train.data\",\"gisette_valid.data\")\n",
    "    x_Data = gisettedata[0:7000, 0:5000]                 #features\n",
    "    y_Data = gisettedata[:, 5000]                        #labels\n",
    "    return x_Data, y_Data\n",
    "\n",
    "# Read qsar data\n",
    "def read_qsar_data(trainingdata):\n",
    "    # read data from file\n",
    "    qsar_data = pd.read_csv(\"biodeg.csv\", delimiter=\";\", header=None)\n",
    "    return qsar_data\n",
    "\n",
    "def qsar_data():\n",
    "    qsardata = read_qsar_data(\"biodeg.csv\")\n",
    "    x_Data = qsardata.iloc[0:1055, 0:41]                  #features\n",
    "    y_Data = qsardata.iloc[0:1055, 41]                    #labels\n",
    "    # making col 41 values numerical\n",
    "    y_Data = np.where(y_Data == \"RB\", 1, -1)    \n",
    "    return x_Data, y_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gisette Dataset Avg % Error</th>\n",
       "      <th>Gisette Dataset Std Dev</th>\n",
       "      <th>QSAR Dataset Avg % Error</th>\n",
       "      <th>QSAR Dataset Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>3.567857</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>22.132701</td>\n",
       "      <td>0.083131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pocket</th>\n",
       "      <td>3.567857</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>15.876777</td>\n",
       "      <td>0.014306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adatron</th>\n",
       "      <td>3.446429</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>21.682464</td>\n",
       "      <td>0.053202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Gisette Dataset Avg % Error  Gisette Dataset Std Dev  \\\n",
       "Perceptron                     3.567857                 0.002654   \n",
       "Pocket                         3.567857                 0.002654   \n",
       "Adatron                        3.446429                 0.002985   \n",
       "\n",
       "            QSAR Dataset Avg % Error  QSAR Dataset Std Dev  \n",
       "Perceptron                 22.132701              0.083131  \n",
       "Pocket                     15.876777              0.014306  \n",
       "Adatron                    21.682464              0.053202  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Algorithms on Gisette Data\n",
    "xData, yData = gisette_data()\n",
    "# Array for errors\n",
    "Gperceptron_total_error = []\n",
    "Gpocket_total_error = []\n",
    "Gadatron_total_error = []\n",
    "for i in range(10):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.4,stratify= yData)\n",
    "    # Perceptron with 50 max_terations and 0.05 learning rate\n",
    "    gpercep = Perceptron(50,0.05)\n",
    "    gpercep.fit(xTrain,yTrain)\n",
    "    g_guesses_p = gpercep.predict(xTest)\n",
    "    g_error_p = (np.sum(g_guesses_p != yTest))/len(yTest)        # Calculate error\n",
    "    Gperceptron_total_error.append(g_error_p)                    # Append in the Array of errors for 10 iterations\n",
    "    #Pocket with 50 max_terations and 0.05 learning rate\n",
    "    gpockt = Pocket(50,0.05)\n",
    "    gpockt.fit(xTrain,yTrain)\n",
    "    g_guesses_pk = gpockt.predict(xTest)\n",
    "    g_error_pk = (np.sum(g_guesses_pk != yTest))/len(yTest)      # Calculate error\n",
    "    Gpocket_total_error.append(g_error_pk)                       # Append in the Array of errors for 10 iterations\n",
    "    #Adatron with 50 max_terations and 0.05 learning rate\n",
    "    Adat = Adatron(50,0.05)\n",
    "    Adat.fit(xTrain,yTrain)\n",
    "    g_guesses_ad = Adat.predict(xTest)\n",
    "    g_error_ad = (np.sum(g_guesses_ad != yTest))/len(yTest)      # Calculate error\n",
    "    Gadatron_total_error.append(g_error_ad)                      # Append in the Array of errors for 10 iterations\n",
    "# Calculate mean and standard deviation for each algorithm\n",
    "Gperceptron_mean=(stat.mean(Gperceptron_total_error))*100        \n",
    "Gperceptron_sd=stat.stdev(Gperceptron_total_error)\n",
    "Gpocket_mean=(stat.mean(Gpocket_total_error))*100\n",
    "Gpocket_sd=stat.stdev(Gpocket_total_error)\n",
    "Gadatron_mean=(stat.mean(Gadatron_total_error))*100\n",
    "Gadatron_sd=stat.stdev(Gadatron_total_error)\n",
    "\n",
    "# Algorithms on QSAR Data\n",
    "xData, yData = qsar_data()\n",
    "#Arrays for errors\n",
    "Qperceptron_total_error = []\n",
    "Qpocket_total_error = []\n",
    "Qadatron_total_error = []\n",
    "for i in range(10):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.4,stratify=yData)\n",
    "    # Perceptron with 50 max_terations and 0.05 learning rate\n",
    "    qpercep = Perceptron(50,0.05)\n",
    "    qpercep.fit(xTrain,yTrain)\n",
    "    q_guesses_p = qpercep.predict(xTest)\n",
    "    q_error_p = (np.sum(q_guesses_p != yTest))/len(yTest)           # Calculate error\n",
    "    Qperceptron_total_error.append(q_error_p)                       # Append in the Array of errors for 10 iterations\n",
    "    #Pocket with 50 max_terations and 0.05 learning rate\n",
    "    qpockt = Pocket(50,0.05)\n",
    "    qpockt.fit(xTrain,yTrain)\n",
    "    q_guesses_pk = qpockt.predict(xTest)\n",
    "    q_error_pk = (np.sum(q_guesses_pk != yTest))/len(yTest)         # Calculate error\n",
    "    Qpocket_total_error.append(q_error_pk)                          # Append in the Array of errors for 10 iterations\n",
    "    #Adatron with 50 max_terations and 0.05 learning rate\n",
    "    qadat = Adatron(50,0.05)\n",
    "    qadat.fit(xTrain,yTrain)\n",
    "    q_guesses_ad = qadat.predict(xTest)\n",
    "    q_error_ad = (np.sum(q_guesses_ad != yTest))/len(yTest)         # Calculate error\n",
    "    Qadatron_total_error.append(q_error_ad)                         # Append in the Array of errors for 10 iterations\n",
    "# Calculate mean and standard deviation for each algorithm\n",
    "Qperceptron_mean=(stat.mean(Qperceptron_total_error))*100\n",
    "Qperceptron_sd=stat.stdev(Qperceptron_total_error)\n",
    "Qpocket_mean=(stat.mean(Qpocket_total_error))*100\n",
    "Qpocket_sd=stat.stdev(Qpocket_total_error)\n",
    "Qadatron_mean=(stat.mean(Qadatron_total_error))*100\n",
    "Qadatron_sd=stat.stdev(Qadatron_total_error)\n",
    "# To display in tabulated format\n",
    "titles = ['Perceptron', 'Pocket', 'Adatron']\n",
    "gisette_avg_error = pd.Series([Gperceptron_mean,\n",
    "                            Gpocket_mean,\n",
    "                            Gadatron_mean],\n",
    "                            index=titles)\n",
    "gisette_std_dev = pd.Series([Gperceptron_sd,\n",
    "                            Gpocket_sd,\n",
    "                            Gadatron_sd],\n",
    "                            index=titles)\n",
    "qsar_avg_error = pd.Series([Qperceptron_mean,\n",
    "                            Qpocket_mean,\n",
    "                            Qadatron_mean],\n",
    "                            index=titles)\n",
    "qsar_std_dev = pd.Series([Qperceptron_sd,\n",
    "                          Qpocket_sd,\n",
    "                          Qadatron_sd],\n",
    "                            index=titles)\n",
    "d = {'Gisette Dataset Avg % Error' : gisette_avg_error, 'Gisette Dataset Std Dev' : gisette_std_dev, 'QSAR Dataset Avg % Error' : qsar_avg_error, 'QSAR Dataset Std Dev': qsar_std_dev}\n",
    "dataframe = pd.DataFrame(d)\n",
    "dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gisette Dataset:**\n",
    "**For max_iterations > 10, perceptron and pocket algorithm have the same error rate and standard deviation. However, for max_iterations < 10, pocket algorithm performs better than the perceptron with less error rate. The error rate for adatron varies drastically and performs best or worse with every run. In this scenario, with maximum iterations= 50 and learning rate= 0.05, Adatron performs best.**\n",
    "\n",
    "**QSAR Dataset:**\n",
    "**The pocket algorithm always performs well on QSAR dataset with error rate ranging from 10 to 20 percent. Most of the times, Adatron performs better than perceptron.**\n",
    "\n",
    "**Thus, in most cases, Pocket algorithm performs better as it stores the weight with minimum error. Standard Deviation is a measure of spread of the data, or how wide it spreads out. Thus, if the standard deviation value is less, it implies there are less outliers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_test_split():\n",
    "Split arrays or matrices into random train and test sets and returns list containg train-test split of inputs.'train_size' and 'test_size' parameters are used to set the splitting percentage. The 'shuffle' parameter is used to shuffle the data before splitting. 'Stratify' parameter is used to split data in a stratified fashion, using this as class labels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Learning Curves \n",
    "\n",
    "Whenever we train a classifier it is useful to know if we have collected a sufficient amount of data for accurate classification.  A good way of determining that is to construct a **learning curve**, which is a plot of classifier performance (i.e. its error) as a function of the number of training examples.  Plot a learning curve for the perceptron algorithm (with bias) using the Gisette dataset.  The x-axis for the plot (number of training examples) should be on a logarithmic scale - something like 10,20,40,80,200,400,800.  Use numbers that are appropriate for the dataset at hand, choosing values that illustrate the variation that you observe.  What can you conclude from the learning curve you have constructed?  Make sure that you use a fixed test set to evaluate performance while varying the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xData, yData = gisette_data()\n",
    "# Split the dataset into training and test sets of 60:40 ratio\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.4)\n",
    "errors = []\n",
    "log_scale=[]\n",
    "for i in range(1,12):\n",
    "    # Varying the training size using train_test_split function\n",
    "    xTrain_new, xTest_new, yTrain_new, yTest_new = train_test_split(xTrain, yTrain, shuffle=True, train_size=2**i, test_size=0.2)\n",
    "    perceptron = Perceptron(100,0.01)\n",
    "    perceptron.fit(xTrain_new,yTrain_new)    \n",
    "    guesses = perceptron.predict(xTest)\n",
    "    calculate_error = (np.sum(guesses != yTest))/len(yTest) \n",
    "    log_scale.append(2**i)\n",
    "    # Append the error list with the above calculated error\n",
    "    errors.append(calculate_error)    \n",
    "plt.semilogx(log_scale, errors, 'b-', basex=2)\n",
    "plt.ylabel('Error in %')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The learning curve shows that the error rate gradually decreases with the increase in the number of training samples. This is because increasing the training data adds information which helps to improve the fit. The error rate fluctuates for training samples < $$2^5$$ and then decreases as the perceptron is not trained enough with sufficient number of training examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Data normalization \n",
    "\n",
    "In this section we will explore the effect of normalizing the data, focusing on normalization of features.  The simplest form of normalization is to scale each feature to be in the range [-1, 1].  We'll call this **scaling**.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "  - Explain how to scale the data to be in the range [-1, 1].\n",
    "  - Compare the accuracy of the perceptron with bias on the original data and the scaled version of the heart dataset.  Does one of them lead to better performance?  Explain why you think this happens.  \n",
    "  - An alternative way of normalizing the data is to **standardize** it:  for each feature subtract the mean and divide by its standard deviation.  What can you say about the range of the resulting features in this case?  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. In order to scale the data to be in the range [-1,1] we use the following expression:\n",
    "                $$x_{normalized} = 2 * (\\frac{x-x_{min}}{x_{max}-x_{min}})-1$$\n",
    "where $$x_{min}$$ and $$x_{max}$$ are the minimum and maximum of the data respectively.**\n",
    "**Thus, for the heart dataset, maximum and minimum values for each feature is calculated and the normalized data is computed** **according to the above mentioned formula.**\n",
    "\n",
    "**2. The accuracy of the perceptron with bias on the scaled version is better than that on original data since the error percentage is in the range of 20 to 28 per cent whereas for original data it is in range of 30 to 40 per cent. And, thus, scaled version performs better as normalization scales the data to range of [-1,1] and this eliminates possible outliers and results in smaller standard deviation.**\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "**3. The range of the resulting features would be different for all the features in case of standardizing the data, whereas in normalize, all the features have range [-1,1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Data</th>\n",
       "      <th>Normalized Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Avg % Error</th>\n",
       "      <td>36.554622</td>\n",
       "      <td>20.336134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std Dev</th>\n",
       "      <td>0.125035</td>\n",
       "      <td>0.072592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Original Data  Normalized Data\n",
       "Avg % Error      36.554622        20.336134\n",
       "Std Dev           0.125035         0.072592"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Dataset\n",
    "Heart_Data=np.genfromtxt(\"processed.cleveland.data.txt\", delimiter=\",\") \n",
    "Heart_DataFrame=pd.DataFrame(Heart_Data)\n",
    "# Mark '?' as Not a Number(NaN)\n",
    "heart_dataset = Heart_DataFrame.replace('?', np.NaN)\n",
    "# drop rows with missing values\n",
    "heart_dataset.dropna(inplace=True, axis='rows')\n",
    "\n",
    "# Normalization of dataset\n",
    "# Calculate minimum of each column in the heart dataset\n",
    "min_data=heart_dataset.min(axis=0)\n",
    "# Calculate maximum of each column in the heart dataset\n",
    "max_data=heart_dataset.max(axis=0)\n",
    "# Formula= 2*((x-min(x))/(max(x)-min(x)))-1\n",
    "normalized_data=(2*((heart_dataset-min_data)/(max_data-min_data)))-1\n",
    "\n",
    "# Result in Tabular Format\n",
    "def compare_data(regular_error, norm_error):\n",
    "    titles = ['Avg % Error', 'Std Dev']\n",
    "    reg = pd.Series([stat.mean(regular_error)*100, stat.stdev(regular_error)], index=titles)\n",
    "    norm = pd.Series([stat.mean(norm_error)*100, stat.stdev(norm_error)], index=titles)\n",
    "    d = {'Original Data' : reg, 'Normalized Data' : norm}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df\n",
    "#Regular error\n",
    "xData = heart_dataset[[0,1,2,3,4,5,6,7,8,9,10,11,12]].values.astype(np.float)\n",
    "yData = heart_dataset[13].values\n",
    "yData = np.where(yData > 0,1,-1)\n",
    "error_regular = []\n",
    "for i in range(10):\n",
    "    # Split the dataset into training and test sets of 60:40 ratio\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.4)\n",
    "    # Call Perceptron Function with max_iterations=100 and learning rate=0.05\n",
    "    perceptronReg = Perceptron(100,0.05)\n",
    "    perceptronReg.fit(xTrain,yTrain)\n",
    "    guessesp = perceptronReg.predict(xTest)\n",
    "    error_reg = (np.sum(guessesp != yTest))/(len(yTest))\n",
    "    error_regular.append(error_reg)\n",
    "#Normalized error\n",
    "xData = normalized_data[[0,1,2,3,4,5,6,7,8,9,10,11,12]].values  # Normalized data\n",
    "yData = heart_dataset[13].values    # Original data\n",
    "yData = np.where(yData > 0,1,-1)\n",
    "error_normalized = []\n",
    "for i in range(10):\n",
    "    # Split the dataset into training and test sets of 60:40 ratio\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.4)\n",
    "    # Call Perceptron Function with max_iterations=100 and learning rate=0.05\n",
    "    perceptronNorm = Perceptron(100,0.05)\n",
    "    perceptronNorm.fit(xTrain,yTrain)\n",
    "    guessesn = perceptronNorm.predict(xTest)\n",
    "    error_norm = (np.sum(guessesn != yTest))/(len(yTest))\n",
    "    error_normalized.append(error_norm)\n",
    "# Result Display\n",
    "compare_data(error_regular, error_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Report\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "\n",
    "Mathematical equations should be written as LaTex equations; the assignment contains multiple examples of both inline formulas (such as the one exemplifying the notation for the norm of a vector $||\\mathbf{x}||$ and those that appear on separate lines, e.g.:\n",
    "\n",
    "$$\n",
    "||\\mathbf{x}|| = \\sqrt{\\mathbf{x}^T \\mathbf{x}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Submit your report as a Jupyter notebook via Canvas.  Running the notebook should generate all the plots and results in your notebook.\n",
    "\n",
    "\n",
    "### Grading \n",
    "\n",
    "Here is what the grade sheet will look like for this assignment.  A few general guidelines for this and future assignments in the course:\n",
    "\n",
    "  * Your answers should be concise and to the point.  We will take off points if that is not the case.\n",
    "  * Always provide a description of the method you used to produce a given result in sufficient detail such that the reader can reproduce your results on the basis of the description.  You can use a few lines of python code or pseudo-code.\n",
    "\n",
    "\n",
    "Grading sheet for the assignment:\n",
    "\n",
    "```\n",
    "Part 1:  60 points.\n",
    "(30 points):  Correct implementation of the classifiers\n",
    "(15 points):  Good protocol for evaluating classifier accuracy; results are provided in a clear and concise way\n",
    "(15 points):  Discussion of the results\n",
    "\n",
    "Part 2:  20 points.\n",
    "(15 points):  Learning curves are correctly generated and displayed in a clear and readable way\n",
    "( 5 points):  Discussion of the results\n",
    "\n",
    "Part 3:  20 points.\n",
    "( 5 points):  How to perform data scaling\n",
    "(10 points):  Comparison of normalized/raw data results; discussion of results\n",
    "( 5 points):  Range of features after standardization\n",
    "```\n",
    "\n",
    "\n",
    "Grading will be based on the following criteria:\n",
    "\n",
    "  * Correctness of answers to math problems\n",
    "  * Math is formatted as LaTex equations\n",
    "  * Correct behavior of the required code\n",
    "  * Easy to understand plots \n",
    "  * Overall readability and organization of the notebook\n",
    "  * Effort in making interesting observations where requested.\n",
    "  * Conciseness.  Points may be taken off if the notebook is overly \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
